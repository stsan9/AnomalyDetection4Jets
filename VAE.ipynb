{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyjet import cluster,DTYPE_PTEPM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = path = '/anomalyvol/data/events_LHCO2020_backgroundMC_Pythia.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2090</th>\n",
       "      <th>2091</th>\n",
       "      <th>2092</th>\n",
       "      <th>2093</th>\n",
       "      <th>2094</th>\n",
       "      <th>2095</th>\n",
       "      <th>2096</th>\n",
       "      <th>2097</th>\n",
       "      <th>2098</th>\n",
       "      <th>2099</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.587869</td>\n",
       "      <td>-2.323472</td>\n",
       "      <td>-2.597121</td>\n",
       "      <td>1.497173</td>\n",
       "      <td>-2.480994</td>\n",
       "      <td>-2.269457</td>\n",
       "      <td>0.848844</td>\n",
       "      <td>-2.465643</td>\n",
       "      <td>-2.096595</td>\n",
       "      <td>0.961511</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.921213</td>\n",
       "      <td>-0.745233</td>\n",
       "      <td>1.018857</td>\n",
       "      <td>0.689363</td>\n",
       "      <td>-0.642245</td>\n",
       "      <td>3.050711</td>\n",
       "      <td>1.999174</td>\n",
       "      <td>-0.343135</td>\n",
       "      <td>-0.322586</td>\n",
       "      <td>1.580572</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.580352</td>\n",
       "      <td>-2.412026</td>\n",
       "      <td>1.680236</td>\n",
       "      <td>0.429869</td>\n",
       "      <td>-0.778697</td>\n",
       "      <td>-1.453413</td>\n",
       "      <td>0.856914</td>\n",
       "      <td>-2.243512</td>\n",
       "      <td>0.217628</td>\n",
       "      <td>0.407344</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.579134</td>\n",
       "      <td>-0.243543</td>\n",
       "      <td>-2.561824</td>\n",
       "      <td>0.312690</td>\n",
       "      <td>-0.283086</td>\n",
       "      <td>-0.281626</td>\n",
       "      <td>0.775053</td>\n",
       "      <td>-2.062494</td>\n",
       "      <td>-1.598718</td>\n",
       "      <td>0.868891</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.644219</td>\n",
       "      <td>-2.457281</td>\n",
       "      <td>-2.670996</td>\n",
       "      <td>0.186128</td>\n",
       "      <td>-1.757650</td>\n",
       "      <td>2.719159</td>\n",
       "      <td>0.346987</td>\n",
       "      <td>-2.318233</td>\n",
       "      <td>-0.155036</td>\n",
       "      <td>0.501437</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0.493004</td>\n",
       "      <td>-0.886976</td>\n",
       "      <td>-0.391002</td>\n",
       "      <td>0.534181</td>\n",
       "      <td>-2.081904</td>\n",
       "      <td>2.548825</td>\n",
       "      <td>0.458036</td>\n",
       "      <td>-1.230976</td>\n",
       "      <td>2.204294</td>\n",
       "      <td>0.639672</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.404833</td>\n",
       "      <td>-0.510012</td>\n",
       "      <td>2.969482</td>\n",
       "      <td>0.300500</td>\n",
       "      <td>-2.248194</td>\n",
       "      <td>1.012964</td>\n",
       "      <td>0.316375</td>\n",
       "      <td>-1.815956</td>\n",
       "      <td>1.011110</td>\n",
       "      <td>0.604675</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.407082</td>\n",
       "      <td>-2.195407</td>\n",
       "      <td>2.632497</td>\n",
       "      <td>0.413497</td>\n",
       "      <td>-1.212703</td>\n",
       "      <td>2.704103</td>\n",
       "      <td>0.578276</td>\n",
       "      <td>-0.291654</td>\n",
       "      <td>1.065150</td>\n",
       "      <td>0.508287</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.573155</td>\n",
       "      <td>-0.755578</td>\n",
       "      <td>0.347725</td>\n",
       "      <td>0.751455</td>\n",
       "      <td>-1.929361</td>\n",
       "      <td>0.759764</td>\n",
       "      <td>0.798534</td>\n",
       "      <td>-0.532149</td>\n",
       "      <td>0.979545</td>\n",
       "      <td>0.941284</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0.481400</td>\n",
       "      <td>-1.687436</td>\n",
       "      <td>-1.485912</td>\n",
       "      <td>0.560682</td>\n",
       "      <td>-0.641398</td>\n",
       "      <td>1.572734</td>\n",
       "      <td>1.772811</td>\n",
       "      <td>-1.698486</td>\n",
       "      <td>-0.681214</td>\n",
       "      <td>3.826682</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 2100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6     \\\n",
       "0    3.587869 -2.323472 -2.597121  1.497173 -2.480994 -2.269457  0.848844   \n",
       "1    0.921213 -0.745233  1.018857  0.689363 -0.642245  3.050711  1.999174   \n",
       "2    0.580352 -2.412026  1.680236  0.429869 -0.778697 -1.453413  0.856914   \n",
       "3    0.579134 -0.243543 -2.561824  0.312690 -0.283086 -0.281626  0.775053   \n",
       "4    0.644219 -2.457281 -2.670996  0.186128 -1.757650  2.719159  0.346987   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "995  0.493004 -0.886976 -0.391002  0.534181 -2.081904  2.548825  0.458036   \n",
       "996  0.404833 -0.510012  2.969482  0.300500 -2.248194  1.012964  0.316375   \n",
       "997  0.407082 -2.195407  2.632497  0.413497 -1.212703  2.704103  0.578276   \n",
       "998  0.573155 -0.755578  0.347725  0.751455 -1.929361  0.759764  0.798534   \n",
       "999  0.481400 -1.687436 -1.485912  0.560682 -0.641398  1.572734  1.772811   \n",
       "\n",
       "         7         8         9     ...  2090  2091  2092  2093  2094  2095  \\\n",
       "0   -2.465643 -2.096595  0.961511  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "1   -0.343135 -0.322586  1.580572  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "2   -2.243512  0.217628  0.407344  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "3   -2.062494 -1.598718  0.868891  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "4   -2.318233 -0.155036  0.501437  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "..        ...       ...       ...  ...   ...   ...   ...   ...   ...   ...   \n",
       "995 -1.230976  2.204294  0.639672  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "996 -1.815956  1.011110  0.604675  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "997 -0.291654  1.065150  0.508287  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "998 -0.532149  0.979545  0.941284  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "999 -1.698486 -0.681214  3.826682  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "\n",
       "     2096  2097  2098  2099  \n",
       "0     0.0   0.0   0.0   0.0  \n",
       "1     0.0   0.0   0.0   0.0  \n",
       "2     0.0   0.0   0.0   0.0  \n",
       "3     0.0   0.0   0.0   0.0  \n",
       "4     0.0   0.0   0.0   0.0  \n",
       "..    ...   ...   ...   ...  \n",
       "995   0.0   0.0   0.0   0.0  \n",
       "996   0.0   0.0   0.0   0.0  \n",
       "997   0.0   0.0   0.0   0.0  \n",
       "998   0.0   0.0   0.0   0.0  \n",
       "999   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[1000 rows x 2100 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_hdf(path,stop=1000) # just read first 1000 events\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_events = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = all_events.shape[0]\n",
    "cols = all_events.shape[1]\n",
    "data = np.zeros((rows, cols // 3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(rows):\n",
    "    pseudojets_input = np.zeros(len([x for x in all_events[i][::3] if x > 0]), dtype=DTYPE_PTEPM)\n",
    "    for j in range(cols // 3):\n",
    "        if (all_events[i][j*3]>0):\n",
    "            pseudojets_input[j]['pT'] = all_events[i][j*3]\n",
    "            pseudojets_input[j]['eta'] = all_events[i][j*3+1]\n",
    "            pseudojets_input[j]['phi'] = all_events[i][j*3+2]\n",
    "        pass\n",
    "    sequence = cluster(pseudojets_input, R=1.0, p=-1)\n",
    "    jets = sequence.inclusive_jets()\n",
    "    for k in range(len(jets)):\n",
    "        data[i][k][0] = jets[k].pt\n",
    "        data[i][k][1] = jets[k].eta\n",
    "        data[i][k][2] = jets[k].phi\n",
    "        data[i][k][3] = jets[k].mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 700, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Lambda, Input, Dense, Flatten, Reshape\n",
    "from keras.models import Model, Sequential\n",
    "from keras import metrics\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    \"\"\"\n",
    "    # Arguments\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "    # Returns\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean = 0 and std = 1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_dim = 32\n",
    "final_dim = 8\n",
    "latent_dim = 2\n",
    "input_shape = (cols // 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder\n",
    "x = Input(shape=input_shape)\n",
    "x_flat = Flatten()(x)\n",
    "h1 = Dense(inter_dim, activation='relu')(x_flat)\n",
    "h2 = Dense(final_dim, activation='relu')(h1)\n",
    "z_mean = Dense(latent_dim)(h2)\n",
    "z_log_sigma = Dense(latent_dim)(h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random sampling\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_sigma])\n",
    "\n",
    "encoder = Model(inputs = x, outputs = z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder\n",
    "decoder_h2 = Dense(final_dim, activation='relu')\n",
    "decoder_h1 = Dense(inter_dim, activation='relu')\n",
    "decoder_mean = Dense(np.prod(input_shape), activation='sigmoid')\n",
    "\n",
    "h2_decoded = decoder_h2(z)\n",
    "h1_decoded = decoder_h1(h2_decoded)\n",
    "x_decoded_mean = decoder_mean(h1_decoded)\n",
    "x_decoded = Reshape(input_shape)(x_decoded_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = Model(inputs = x, outputs = x_decoded, name = 'vae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(x, y):\n",
    "    xent_loss = metrics.binary_crossentropy(K.flatten(x), K.flatten(y))\n",
    "    kl_loss = - 0.5 * K.mean(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma), axis=-1)\n",
    "    return xent_loss + kl_loss\n",
    "\n",
    "vae.compile(optimizer='adam', loss=vae_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 700, 4)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 2800)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 32)           89632       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 8)            264         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2)            18          dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 2)            18          dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 2)            0           dense_3[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 8)            24          lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 32)           288         dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 2800)         92400       dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 700, 4)       0           dense_7[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 182,644\n",
      "Trainable params: 182,644\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data[0:int(len(data) * 0.8)]\n",
    "x_val = data[int(len(data) * 0.8):]\n",
    "batch_size = 100\n",
    "epochs = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 700, 4)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/150\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 38634135.4211 - val_loss: 249.6192\n",
      "Epoch 2/150\n",
      "800/800 [==============================] - 0s 119us/step - loss: 291.6356 - val_loss: 402.7793\n",
      "Epoch 3/150\n",
      "800/800 [==============================] - 0s 128us/step - loss: 389.7881 - val_loss: 1724.2498\n",
      "Epoch 4/150\n",
      "800/800 [==============================] - 0s 130us/step - loss: 422.2971 - val_loss: 554.9063\n",
      "Epoch 5/150\n",
      "800/800 [==============================] - 0s 111us/step - loss: 223.8300 - val_loss: 215.8408\n",
      "Epoch 6/150\n",
      "800/800 [==============================] - 0s 117us/step - loss: 182.0653 - val_loss: 173.9011\n",
      "Epoch 7/150\n",
      "800/800 [==============================] - 0s 122us/step - loss: 159.9630 - val_loss: 153.5538\n",
      "Epoch 8/150\n",
      "800/800 [==============================] - 0s 120us/step - loss: 145.0281 - val_loss: 139.6160\n",
      "Epoch 9/150\n",
      "800/800 [==============================] - 0s 119us/step - loss: 132.0679 - val_loss: 128.2035\n",
      "Epoch 10/150\n",
      "800/800 [==============================] - 0s 126us/step - loss: 120.4211 - val_loss: 117.6097\n",
      "Epoch 11/150\n",
      "800/800 [==============================] - 0s 124us/step - loss: 110.1683 - val_loss: 107.8485\n",
      "Epoch 12/150\n",
      "800/800 [==============================] - 0s 127us/step - loss: 100.3261 - val_loss: 98.1955\n",
      "Epoch 13/150\n",
      "800/800 [==============================] - 0s 132us/step - loss: 91.0018 - val_loss: 89.5660\n",
      "Epoch 14/150\n",
      "800/800 [==============================] - 0s 127us/step - loss: 82.6663 - val_loss: 81.4078\n",
      "Epoch 15/150\n",
      "800/800 [==============================] - 0s 134us/step - loss: 75.2462 - val_loss: 74.6634\n",
      "Epoch 16/150\n",
      "800/800 [==============================] - 0s 98us/step - loss: 68.4781 - val_loss: 68.0706\n",
      "Epoch 17/150\n",
      "800/800 [==============================] - 0s 124us/step - loss: 62.7426 - val_loss: 62.4522\n",
      "Epoch 18/150\n",
      "800/800 [==============================] - 0s 112us/step - loss: 57.3412 - val_loss: 57.2157\n",
      "Epoch 19/150\n",
      "800/800 [==============================] - 0s 102us/step - loss: 52.4270 - val_loss: 52.2224\n",
      "Epoch 20/150\n",
      "800/800 [==============================] - 0s 117us/step - loss: 47.7896 - val_loss: 47.7456\n",
      "Epoch 21/150\n",
      "800/800 [==============================] - 0s 99us/step - loss: 43.5930 - val_loss: 43.6283\n",
      "Epoch 22/150\n",
      "800/800 [==============================] - 0s 108us/step - loss: 39.4353 - val_loss: 39.6669\n",
      "Epoch 23/150\n",
      "800/800 [==============================] - 0s 122us/step - loss: 35.8438 - val_loss: 35.9800\n",
      "Epoch 24/150\n",
      "800/800 [==============================] - 0s 104us/step - loss: 32.3138 - val_loss: 32.5302\n",
      "Epoch 25/150\n",
      "800/800 [==============================] - 0s 99us/step - loss: 29.2774 - val_loss: 29.8937\n",
      "Epoch 26/150\n",
      "800/800 [==============================] - 0s 116us/step - loss: 26.3366 - val_loss: 26.8886\n",
      "Epoch 27/150\n",
      "800/800 [==============================] - 0s 116us/step - loss: 23.6100 - val_loss: 24.3094\n",
      "Epoch 28/150\n",
      "800/800 [==============================] - 0s 114us/step - loss: 21.0200 - val_loss: 21.8315\n",
      "Epoch 29/150\n",
      "800/800 [==============================] - 0s 108us/step - loss: 18.7254 - val_loss: 19.2421\n",
      "Epoch 30/150\n",
      "800/800 [==============================] - 0s 122us/step - loss: 16.6506 - val_loss: 17.2786\n",
      "Epoch 31/150\n",
      "800/800 [==============================] - 0s 109us/step - loss: 14.4902 - val_loss: 15.4052\n",
      "Epoch 32/150\n",
      "800/800 [==============================] - 0s 94us/step - loss: 12.6909 - val_loss: 13.5039\n",
      "Epoch 33/150\n",
      "800/800 [==============================] - 0s 106us/step - loss: 10.8852 - val_loss: 11.7012\n",
      "Epoch 34/150\n",
      "800/800 [==============================] - 0s 119us/step - loss: 9.3247 - val_loss: 10.1895\n",
      "Epoch 35/150\n",
      "800/800 [==============================] - 0s 108us/step - loss: 7.9159 - val_loss: 8.6916\n",
      "Epoch 36/150\n",
      "800/800 [==============================] - 0s 123us/step - loss: 6.5251 - val_loss: 7.2082\n",
      "Epoch 37/150\n",
      "800/800 [==============================] - 0s 114us/step - loss: 5.2554 - val_loss: 6.0846\n",
      "Epoch 38/150\n",
      "800/800 [==============================] - 0s 117us/step - loss: 4.0271 - val_loss: 4.9625\n",
      "Epoch 39/150\n",
      "800/800 [==============================] - 0s 116us/step - loss: 3.0849 - val_loss: 3.9066\n",
      "Epoch 40/150\n",
      "800/800 [==============================] - 0s 115us/step - loss: 2.0941 - val_loss: 2.8128\n",
      "Epoch 41/150\n",
      "800/800 [==============================] - 0s 100us/step - loss: 1.1497 - val_loss: 1.9459\n",
      "Epoch 42/150\n",
      "800/800 [==============================] - 0s 113us/step - loss: 0.1752 - val_loss: 1.3356\n",
      "Epoch 43/150\n",
      "800/800 [==============================] - 0s 116us/step - loss: -0.5316 - val_loss: 0.3350\n",
      "Epoch 44/150\n",
      "800/800 [==============================] - 0s 110us/step - loss: -1.2029 - val_loss: -0.6150\n",
      "Epoch 45/150\n",
      "800/800 [==============================] - 0s 124us/step - loss: -1.8428 - val_loss: -1.2291\n",
      "Epoch 46/150\n",
      "800/800 [==============================] - 0s 99us/step - loss: -2.6029 - val_loss: -1.8934\n",
      "Epoch 47/150\n",
      "800/800 [==============================] - 0s 109us/step - loss: -3.2589 - val_loss: -2.4960\n",
      "Epoch 48/150\n",
      "800/800 [==============================] - 0s 106us/step - loss: -3.7764 - val_loss: -3.0553\n",
      "Epoch 49/150\n",
      "800/800 [==============================] - 0s 143us/step - loss: -4.3548 - val_loss: -3.7468\n",
      "Epoch 50/150\n",
      "800/800 [==============================] - 0s 101us/step - loss: -4.8229 - val_loss: -4.0335\n",
      "Epoch 51/150\n",
      "800/800 [==============================] - 0s 118us/step - loss: -5.3624 - val_loss: -4.5201\n",
      "Epoch 52/150\n",
      "800/800 [==============================] - 0s 105us/step - loss: -5.7803 - val_loss: -5.1203\n",
      "Epoch 53/150\n",
      "800/800 [==============================] - 0s 114us/step - loss: -6.2816 - val_loss: -5.6671\n",
      "Epoch 54/150\n",
      "800/800 [==============================] - 0s 113us/step - loss: -6.6416 - val_loss: -6.1139\n",
      "Epoch 55/150\n",
      "800/800 [==============================] - 0s 109us/step - loss: -7.0066 - val_loss: -6.5084\n",
      "Epoch 56/150\n",
      "800/800 [==============================] - 0s 122us/step - loss: -7.4408 - val_loss: -6.9234\n",
      "Epoch 57/150\n",
      "800/800 [==============================] - 0s 110us/step - loss: -7.7743 - val_loss: -7.2850\n",
      "Epoch 58/150\n",
      "800/800 [==============================] - 0s 107us/step - loss: -8.0292 - val_loss: -7.6315\n",
      "Epoch 59/150\n",
      "800/800 [==============================] - 0s 104us/step - loss: -8.4121 - val_loss: -7.8788\n",
      "Epoch 60/150\n",
      "800/800 [==============================] - 0s 113us/step - loss: -8.6398 - val_loss: -8.1933\n",
      "Epoch 61/150\n",
      "800/800 [==============================] - 0s 107us/step - loss: -8.9233 - val_loss: -8.4814\n",
      "Epoch 62/150\n",
      "800/800 [==============================] - 0s 116us/step - loss: -9.1224 - val_loss: -8.6853\n",
      "Epoch 63/150\n",
      "800/800 [==============================] - 0s 125us/step - loss: -9.3079 - val_loss: -8.9000\n",
      "Epoch 64/150\n",
      "800/800 [==============================] - 0s 130us/step - loss: -9.5458 - val_loss: -9.1577\n",
      "Epoch 65/150\n",
      "800/800 [==============================] - 0s 118us/step - loss: -9.6783 - val_loss: -9.2409\n",
      "Epoch 66/150\n",
      "800/800 [==============================] - 0s 115us/step - loss: -9.8493 - val_loss: -9.5089\n",
      "Epoch 67/150\n",
      "800/800 [==============================] - 0s 111us/step - loss: -10.0180 - val_loss: -9.6196\n",
      "Epoch 68/150\n",
      "800/800 [==============================] - 0s 109us/step - loss: -10.1668 - val_loss: -9.7897\n",
      "Epoch 69/150\n",
      "800/800 [==============================] - 0s 120us/step - loss: -10.2818 - val_loss: -9.9524\n",
      "Epoch 70/150\n",
      "800/800 [==============================] - 0s 105us/step - loss: -10.3987 - val_loss: -10.0363\n",
      "Epoch 71/150\n",
      "800/800 [==============================] - 0s 129us/step - loss: -10.4615 - val_loss: -10.1855\n",
      "Epoch 72/150\n",
      "800/800 [==============================] - 0s 133us/step - loss: -10.5682 - val_loss: -10.3080\n",
      "Epoch 73/150\n",
      "800/800 [==============================] - 0s 115us/step - loss: -10.7107 - val_loss: -10.4471\n",
      "Epoch 74/150\n",
      "800/800 [==============================] - 0s 97us/step - loss: -10.7778 - val_loss: -10.4947\n",
      "Epoch 75/150\n",
      "800/800 [==============================] - 0s 110us/step - loss: -10.8413 - val_loss: -10.6209\n",
      "Epoch 76/150\n",
      "800/800 [==============================] - 0s 113us/step - loss: -10.9334 - val_loss: -10.7042\n",
      "Epoch 77/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 0s 112us/step - loss: -10.9425 - val_loss: -10.7185\n",
      "Epoch 78/150\n",
      "800/800 [==============================] - 0s 113us/step - loss: -11.0467 - val_loss: -10.8339\n",
      "Epoch 79/150\n",
      "800/800 [==============================] - 0s 108us/step - loss: -11.1273 - val_loss: -10.9416\n",
      "Epoch 80/150\n",
      "800/800 [==============================] - 0s 117us/step - loss: -11.1489 - val_loss: -11.0091\n",
      "Epoch 81/150\n",
      "800/800 [==============================] - 0s 114us/step - loss: -11.2414 - val_loss: -11.1079\n",
      "Epoch 82/150\n",
      "800/800 [==============================] - 0s 118us/step - loss: -11.3062 - val_loss: -11.1705\n",
      "Epoch 83/150\n",
      "800/800 [==============================] - 0s 117us/step - loss: -11.3329 - val_loss: -11.2069\n",
      "Epoch 84/150\n",
      "800/800 [==============================] - 0s 112us/step - loss: -11.3766 - val_loss: -11.2510\n",
      "Epoch 85/150\n",
      "800/800 [==============================] - 0s 123us/step - loss: -11.4137 - val_loss: -11.3143\n",
      "Epoch 86/150\n",
      "800/800 [==============================] - 0s 121us/step - loss: -11.4858 - val_loss: -11.3790\n",
      "Epoch 87/150\n",
      "800/800 [==============================] - 0s 93us/step - loss: -11.5021 - val_loss: -11.4170\n",
      "Epoch 88/150\n",
      "800/800 [==============================] - 0s 124us/step - loss: -11.5598 - val_loss: -11.4699\n",
      "Epoch 89/150\n",
      "800/800 [==============================] - 0s 110us/step - loss: -11.5984 - val_loss: -11.5264\n",
      "Epoch 90/150\n",
      "800/800 [==============================] - 0s 121us/step - loss: -11.6174 - val_loss: -11.5643\n",
      "Epoch 91/150\n",
      "800/800 [==============================] - 0s 117us/step - loss: -11.6718 - val_loss: -11.6274\n",
      "Epoch 92/150\n",
      "800/800 [==============================] - 0s 117us/step - loss: -11.7073 - val_loss: -11.6808\n",
      "Epoch 93/150\n",
      "800/800 [==============================] - 0s 108us/step - loss: -11.7389 - val_loss: -11.7162\n",
      "Epoch 94/150\n",
      "800/800 [==============================] - 0s 128us/step - loss: -11.7727 - val_loss: -11.7326\n",
      "Epoch 95/150\n",
      "800/800 [==============================] - 0s 118us/step - loss: -11.8050 - val_loss: -11.7531\n",
      "Epoch 96/150\n",
      "800/800 [==============================] - 0s 115us/step - loss: -11.8258 - val_loss: -11.8145\n",
      "Epoch 97/150\n",
      "800/800 [==============================] - 0s 119us/step - loss: -11.8605 - val_loss: -11.8631\n",
      "Epoch 98/150\n",
      "800/800 [==============================] - 0s 118us/step - loss: -11.8854 - val_loss: -11.8919\n",
      "Epoch 99/150\n",
      "800/800 [==============================] - 0s 122us/step - loss: -11.9049 - val_loss: -11.9241\n",
      "Epoch 100/150\n",
      "800/800 [==============================] - 0s 118us/step - loss: -11.9342 - val_loss: -11.9683\n",
      "Epoch 101/150\n",
      "800/800 [==============================] - 0s 123us/step - loss: -11.9680 - val_loss: -11.9829\n",
      "Epoch 102/150\n",
      "800/800 [==============================] - 0s 118us/step - loss: -11.9963 - val_loss: -12.0224\n",
      "Epoch 103/150\n",
      "800/800 [==============================] - 0s 118us/step - loss: -12.0265 - val_loss: -12.0688\n",
      "Epoch 104/150\n",
      "800/800 [==============================] - 0s 122us/step - loss: -12.0464 - val_loss: -12.0841\n",
      "Epoch 105/150\n",
      "800/800 [==============================] - 0s 109us/step - loss: -12.0712 - val_loss: -12.1283\n",
      "Epoch 106/150\n",
      "800/800 [==============================] - 0s 107us/step - loss: -12.0995 - val_loss: -12.1560\n",
      "Epoch 107/150\n",
      "800/800 [==============================] - 0s 95us/step - loss: -12.1193 - val_loss: -12.1818\n",
      "Epoch 108/150\n",
      "800/800 [==============================] - 0s 121us/step - loss: -12.1451 - val_loss: -12.2118\n",
      "Epoch 109/150\n",
      "800/800 [==============================] - 0s 115us/step - loss: -12.1645 - val_loss: -12.2409\n",
      "Epoch 110/150\n",
      "800/800 [==============================] - 0s 131us/step - loss: -12.1823 - val_loss: -12.2732\n",
      "Epoch 111/150\n",
      "800/800 [==============================] - 0s 112us/step - loss: -12.2106 - val_loss: -12.3003\n",
      "Epoch 112/150\n",
      "800/800 [==============================] - 0s 115us/step - loss: -12.2334 - val_loss: -12.3251\n",
      "Epoch 113/150\n",
      "800/800 [==============================] - 0s 106us/step - loss: -12.2534 - val_loss: -12.3439\n",
      "Epoch 114/150\n",
      "800/800 [==============================] - 0s 107us/step - loss: -12.2736 - val_loss: -12.3781\n",
      "Epoch 115/150\n",
      "800/800 [==============================] - 0s 116us/step - loss: -12.2937 - val_loss: -12.4042\n",
      "Epoch 116/150\n",
      "800/800 [==============================] - 0s 111us/step - loss: -12.3183 - val_loss: -12.4262\n",
      "Epoch 117/150\n",
      "800/800 [==============================] - 0s 121us/step - loss: -12.3369 - val_loss: -12.4517\n",
      "Epoch 118/150\n",
      "800/800 [==============================] - 0s 103us/step - loss: -12.3567 - val_loss: -12.4764\n",
      "Epoch 119/150\n",
      "800/800 [==============================] - 0s 109us/step - loss: -12.3755 - val_loss: -12.5012\n",
      "Epoch 120/150\n",
      "800/800 [==============================] - 0s 112us/step - loss: -12.3950 - val_loss: -12.5256\n",
      "Epoch 121/150\n",
      "800/800 [==============================] - 0s 116us/step - loss: -12.4166 - val_loss: -12.5479\n",
      "Epoch 122/150\n",
      "800/800 [==============================] - 0s 132us/step - loss: -12.4331 - val_loss: -12.5715\n",
      "Epoch 123/150\n",
      "800/800 [==============================] - 0s 108us/step - loss: -12.4548 - val_loss: -12.5918\n",
      "Epoch 124/150\n",
      "800/800 [==============================] - 0s 117us/step - loss: -12.4689 - val_loss: -12.6172\n",
      "Epoch 125/150\n",
      "800/800 [==============================] - 0s 106us/step - loss: -12.4910 - val_loss: -12.6377\n",
      "Epoch 126/150\n",
      "800/800 [==============================] - 0s 113us/step - loss: -12.5102 - val_loss: -12.6591\n",
      "Epoch 127/150\n",
      "800/800 [==============================] - 0s 122us/step - loss: -12.5278 - val_loss: -12.6804\n",
      "Epoch 128/150\n",
      "800/800 [==============================] - 0s 110us/step - loss: -12.5448 - val_loss: -12.7012\n",
      "Epoch 129/150\n",
      "800/800 [==============================] - 0s 113us/step - loss: -12.5628 - val_loss: -12.7233\n",
      "Epoch 130/150\n",
      "800/800 [==============================] - 0s 110us/step - loss: -12.5802 - val_loss: -12.7443\n",
      "Epoch 131/150\n",
      "800/800 [==============================] - 0s 119us/step - loss: -12.5974 - val_loss: -12.7649\n",
      "Epoch 132/150\n",
      "800/800 [==============================] - 0s 124us/step - loss: -12.6159 - val_loss: -12.7841\n",
      "Epoch 133/150\n",
      "800/800 [==============================] - 0s 120us/step - loss: -12.6329 - val_loss: -12.8026\n",
      "Epoch 134/150\n",
      "800/800 [==============================] - 0s 115us/step - loss: -12.6476 - val_loss: -12.8231\n",
      "Epoch 135/150\n",
      "800/800 [==============================] - 0s 121us/step - loss: -12.6662 - val_loss: -12.8416\n",
      "Epoch 136/150\n",
      "800/800 [==============================] - 0s 115us/step - loss: -12.6813 - val_loss: -12.8620\n",
      "Epoch 137/150\n",
      "800/800 [==============================] - 0s 110us/step - loss: -12.6997 - val_loss: -12.8794\n",
      "Epoch 138/150\n",
      "800/800 [==============================] - 0s 116us/step - loss: -12.7150 - val_loss: -12.8977\n",
      "Epoch 139/150\n",
      "800/800 [==============================] - 0s 124us/step - loss: -12.7286 - val_loss: -12.9187\n",
      "Epoch 140/150\n",
      "800/800 [==============================] - 0s 126us/step - loss: -12.7461 - val_loss: -12.9371\n",
      "Epoch 141/150\n",
      "800/800 [==============================] - 0s 119us/step - loss: -12.7634 - val_loss: -12.9544\n",
      "Epoch 142/150\n",
      "800/800 [==============================] - 0s 108us/step - loss: -12.7791 - val_loss: -12.9709\n",
      "Epoch 143/150\n",
      "800/800 [==============================] - 0s 120us/step - loss: -12.7938 - val_loss: -12.9895\n",
      "Epoch 144/150\n",
      "800/800 [==============================] - 0s 134us/step - loss: -12.8098 - val_loss: -13.0058\n",
      "Epoch 145/150\n",
      "800/800 [==============================] - 0s 113us/step - loss: -12.8234 - val_loss: -13.0238\n",
      "Epoch 146/150\n",
      "800/800 [==============================] - 0s 113us/step - loss: -12.8400 - val_loss: -13.0399\n",
      "Epoch 147/150\n",
      "800/800 [==============================] - 0s 119us/step - loss: -12.8549 - val_loss: -13.0567\n",
      "Epoch 148/150\n",
      "800/800 [==============================] - 0s 111us/step - loss: -12.8691 - val_loss: -13.0742\n",
      "Epoch 149/150\n",
      "800/800 [==============================] - 0s 129us/step - loss: -12.8840 - val_loss: -13.0903\n",
      "Epoch 150/150\n",
      "800/800 [==============================] - 0s 96us/step - loss: -12.8981 - val_loss: -13.1073\n"
     ]
    }
   ],
   "source": [
    "hist = vae.fit(x_train, x_train,\n",
    "               shuffle=True,\n",
    "               epochs=epochs,\n",
    "               batch_size=batch_size,\n",
    "               validation_data=(x_val, x_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
