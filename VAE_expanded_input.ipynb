{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyjet import cluster,DTYPE_PTEPM\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/anomalyvol/data/events_LHCO2020_backgroundMC_Pythia.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2090</th>\n",
       "      <th>2091</th>\n",
       "      <th>2092</th>\n",
       "      <th>2093</th>\n",
       "      <th>2094</th>\n",
       "      <th>2095</th>\n",
       "      <th>2096</th>\n",
       "      <th>2097</th>\n",
       "      <th>2098</th>\n",
       "      <th>2099</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.587869</td>\n",
       "      <td>-2.323472</td>\n",
       "      <td>-2.597121</td>\n",
       "      <td>1.497173</td>\n",
       "      <td>-2.480994</td>\n",
       "      <td>-2.269457</td>\n",
       "      <td>0.848844</td>\n",
       "      <td>-2.465643</td>\n",
       "      <td>-2.096595</td>\n",
       "      <td>0.961511</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.921213</td>\n",
       "      <td>-0.745233</td>\n",
       "      <td>1.018857</td>\n",
       "      <td>0.689363</td>\n",
       "      <td>-0.642245</td>\n",
       "      <td>3.050711</td>\n",
       "      <td>1.999174</td>\n",
       "      <td>-0.343135</td>\n",
       "      <td>-0.322586</td>\n",
       "      <td>1.580572</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.580352</td>\n",
       "      <td>-2.412026</td>\n",
       "      <td>1.680236</td>\n",
       "      <td>0.429869</td>\n",
       "      <td>-0.778697</td>\n",
       "      <td>-1.453413</td>\n",
       "      <td>0.856914</td>\n",
       "      <td>-2.243512</td>\n",
       "      <td>0.217628</td>\n",
       "      <td>0.407344</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.579134</td>\n",
       "      <td>-0.243543</td>\n",
       "      <td>-2.561824</td>\n",
       "      <td>0.312690</td>\n",
       "      <td>-0.283086</td>\n",
       "      <td>-0.281626</td>\n",
       "      <td>0.775053</td>\n",
       "      <td>-2.062494</td>\n",
       "      <td>-1.598718</td>\n",
       "      <td>0.868891</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.644219</td>\n",
       "      <td>-2.457281</td>\n",
       "      <td>-2.670996</td>\n",
       "      <td>0.186128</td>\n",
       "      <td>-1.757650</td>\n",
       "      <td>2.719159</td>\n",
       "      <td>0.346987</td>\n",
       "      <td>-2.318233</td>\n",
       "      <td>-0.155036</td>\n",
       "      <td>0.501437</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>0.578935</td>\n",
       "      <td>-1.251851</td>\n",
       "      <td>-2.358805</td>\n",
       "      <td>0.354713</td>\n",
       "      <td>-0.180199</td>\n",
       "      <td>0.731343</td>\n",
       "      <td>1.048696</td>\n",
       "      <td>-1.031479</td>\n",
       "      <td>-0.956550</td>\n",
       "      <td>0.968440</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>0.222244</td>\n",
       "      <td>-0.376612</td>\n",
       "      <td>1.071711</td>\n",
       "      <td>0.967087</td>\n",
       "      <td>-2.237488</td>\n",
       "      <td>-1.870470</td>\n",
       "      <td>0.377234</td>\n",
       "      <td>-1.306800</td>\n",
       "      <td>2.642717</td>\n",
       "      <td>1.566379</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>1.479221</td>\n",
       "      <td>-1.356321</td>\n",
       "      <td>1.618150</td>\n",
       "      <td>2.503858</td>\n",
       "      <td>-1.079203</td>\n",
       "      <td>1.983957</td>\n",
       "      <td>5.551156</td>\n",
       "      <td>-1.024912</td>\n",
       "      <td>1.965017</td>\n",
       "      <td>1.576914</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>0.669072</td>\n",
       "      <td>-0.404555</td>\n",
       "      <td>0.827083</td>\n",
       "      <td>0.600054</td>\n",
       "      <td>-0.367350</td>\n",
       "      <td>1.268752</td>\n",
       "      <td>5.808274</td>\n",
       "      <td>-0.563534</td>\n",
       "      <td>-2.600536</td>\n",
       "      <td>7.719838</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>0.277209</td>\n",
       "      <td>-2.312783</td>\n",
       "      <td>2.864564</td>\n",
       "      <td>0.770924</td>\n",
       "      <td>-2.045005</td>\n",
       "      <td>-1.648299</td>\n",
       "      <td>0.483227</td>\n",
       "      <td>-1.505444</td>\n",
       "      <td>-0.476310</td>\n",
       "      <td>0.651324</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows Ã— 2100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6     \\\n",
       "0     3.587869 -2.323472 -2.597121  1.497173 -2.480994 -2.269457  0.848844   \n",
       "1     0.921213 -0.745233  1.018857  0.689363 -0.642245  3.050711  1.999174   \n",
       "2     0.580352 -2.412026  1.680236  0.429869 -0.778697 -1.453413  0.856914   \n",
       "3     0.579134 -0.243543 -2.561824  0.312690 -0.283086 -0.281626  0.775053   \n",
       "4     0.644219 -2.457281 -2.670996  0.186128 -1.757650  2.719159  0.346987   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4995  0.578935 -1.251851 -2.358805  0.354713 -0.180199  0.731343  1.048696   \n",
       "4996  0.222244 -0.376612  1.071711  0.967087 -2.237488 -1.870470  0.377234   \n",
       "4997  1.479221 -1.356321  1.618150  2.503858 -1.079203  1.983957  5.551156   \n",
       "4998  0.669072 -0.404555  0.827083  0.600054 -0.367350  1.268752  5.808274   \n",
       "4999  0.277209 -2.312783  2.864564  0.770924 -2.045005 -1.648299  0.483227   \n",
       "\n",
       "          7         8         9     ...  2090  2091  2092  2093  2094  2095  \\\n",
       "0    -2.465643 -2.096595  0.961511  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "1    -0.343135 -0.322586  1.580572  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "2    -2.243512  0.217628  0.407344  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "3    -2.062494 -1.598718  0.868891  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "4    -2.318233 -0.155036  0.501437  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "...        ...       ...       ...  ...   ...   ...   ...   ...   ...   ...   \n",
       "4995 -1.031479 -0.956550  0.968440  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "4996 -1.306800  2.642717  1.566379  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "4997 -1.024912  1.965017  1.576914  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "4998 -0.563534 -2.600536  7.719838  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "4999 -1.505444 -0.476310  0.651324  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "\n",
       "      2096  2097  2098  2099  \n",
       "0      0.0   0.0   0.0   0.0  \n",
       "1      0.0   0.0   0.0   0.0  \n",
       "2      0.0   0.0   0.0   0.0  \n",
       "3      0.0   0.0   0.0   0.0  \n",
       "4      0.0   0.0   0.0   0.0  \n",
       "...    ...   ...   ...   ...  \n",
       "4995   0.0   0.0   0.0   0.0  \n",
       "4996   0.0   0.0   0.0   0.0  \n",
       "4997   0.0   0.0   0.0   0.0  \n",
       "4998   0.0   0.0   0.0   0.0  \n",
       "4999   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5000 rows x 2100 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_hdf(path,stop=5000) # just read first 100000 events\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_events = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = all_events.shape[0]\n",
    "cols = all_events.shape[1]\n",
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(rows):\n",
    "    pseudojets_input = np.zeros(len([x for x in all_events[i][::3] if x > 0]), dtype=DTYPE_PTEPM)\n",
    "    for j in range(cols // 3):\n",
    "        if (all_events[i][j*3]>0):\n",
    "            pseudojets_input[j]['pT'] = all_events[i][j*3]\n",
    "            pseudojets_input[j]['eta'] = all_events[i][j*3+1]\n",
    "            pseudojets_input[j]['phi'] = all_events[i][j*3+2]\n",
    "        pass\n",
    "    sequence = cluster(pseudojets_input, R=1.0, p=-1)\n",
    "    jets = sequence.inclusive_jets()\n",
    "    for k in range(len(jets)):\n",
    "        jet_parts = [list(l) for l in jets[k].constituents_array()]\n",
    "        \n",
    "        # pad jets\n",
    "        if (len(jet_parts) < 100):\n",
    "            pads = 100 - len(jet_parts)\n",
    "            for p in range(pads):\n",
    "                jet_parts.append([0,0,0,0])\n",
    "        else:\n",
    "            jet_parts = jet_parts[0:100]  # just get 100 particles for this jet (arbitrary)\n",
    "    \n",
    "        data.append(jet_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data into format easier for scaling with sklearn ([n_particles, 4])\n",
    "d = []\n",
    "for i in range(len(data)):\n",
    "    for j in range(100):\n",
    "        d.append(data[i][j])\n",
    "data = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7364100, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array(data)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data (rounding by 100)\n",
    "x_train = data[0:int(round(len(data) * 0.8, -2))]\n",
    "x_val = data[int(round(len(data) * 0.8, -2)):int(round(len(data) * 0.9, -2))]\n",
    "x_test = data[int(round(len(data) * 0.9, -2)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regularize\n",
    "scaler = preprocessing.StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_val = scaler.transform(x_val)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.reshape(x_train, (-1,100,4))\n",
    "x_val = np.reshape(x_val, (-1,100,4))\n",
    "x_test = np.reshape(x_test, (-1,100,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7364, 100, 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Lambda, Input, Dense, Flatten, Reshape\n",
    "from keras.models import Model, Sequential\n",
    "from keras import metrics, losses\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    \"\"\"\n",
    "    # Arguments\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "    # Returns\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean = 0 and std = 1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_dim_1 = 256\n",
    "inter_dim_2 = 128\n",
    "final_dim = 64\n",
    "latent_dim = 32\n",
    "input_dim = (100,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder\n",
    "x = Input(shape=input_dim)\n",
    "x_flat = Flatten()(x)\n",
    "h1 = Dense(inter_dim_1, activation='relu')(x_flat)\n",
    "h2 = Dense(inter_dim_2, activation='relu')(h1)\n",
    "h3 = Dense(final_dim, activation='relu')(h2)\n",
    "z_mean = Dense(latent_dim)(h3)\n",
    "z_log_sigma = Dense(latent_dim)(h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 100, 4])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random sampling\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_sigma])\n",
    "\n",
    "encoder = Model(inputs = x, outputs = z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder\n",
    "decoder_h3 = Dense(final_dim, activation='relu')\n",
    "decoder_h2 = Dense(inter_dim_2, activation='relu')\n",
    "decoder_h1 = Dense(inter_dim_1, activation='relu')\n",
    "decoder_mean = Dense(np.prod(input_dim), activation='linear')\n",
    "\n",
    "h3_decoded = decoder_h3(z)\n",
    "h2_decoded = decoder_h2(h3_decoded)\n",
    "h1_decoded = decoder_h1(h2_decoded)\n",
    "x_decoded_mean = decoder_mean(h1_decoded)\n",
    "x_decoded = Reshape(input_dim)(x_decoded_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = Model(inputs = x, outputs = x_decoded_mean, name = 'vae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = keras.losses.MeanSquaredError()\n",
    "def vae_loss(x, y):\n",
    "    mse_loss = mse(K.flatten(x), K.flatten(y))\n",
    "    kl_loss = - 0.5 * K.mean(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma), axis=-1)\n",
    "    return mse_loss + kl_loss\n",
    "\n",
    "vae.compile(optimizer='adam', loss=vae_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 100, 4)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 400)          0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 400)          160400      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          102656      dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          65792       dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 128)          32896       dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 128)          32896       dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 128)          0           dense_4[0][0]                    \n",
      "                                                                 dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 256)          33024       lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 256)          65792       dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 400)          102800      dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 400)          160400      dense_8[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 756,656\n",
      "Trainable params: 756,656\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58913, 100, 4)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_callbacks = [\n",
    "    keras.callbacks.EarlyStopping(patience=10)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 58913 samples, validate on 7364 samples\n",
      "Epoch 1/100\n",
      "58913/58913 [==============================] - 2s 28us/step - loss: 1.0285 - val_loss: 0.9778\n",
      "Epoch 2/100\n",
      "58913/58913 [==============================] - 1s 14us/step - loss: 1.0000 - val_loss: 0.9767\n",
      "Epoch 3/100\n",
      "58913/58913 [==============================] - 1s 14us/step - loss: 0.9994 - val_loss: 0.9765\n",
      "Epoch 4/100\n",
      "58913/58913 [==============================] - 1s 15us/step - loss: 0.9993 - val_loss: 0.9764\n",
      "Epoch 5/100\n",
      "58913/58913 [==============================] - 1s 14us/step - loss: 0.9993 - val_loss: 0.9764\n",
      "Epoch 6/100\n",
      "58913/58913 [==============================] - 1s 14us/step - loss: 0.9992 - val_loss: 0.9764\n",
      "Epoch 7/100\n",
      "58913/58913 [==============================] - 1s 14us/step - loss: 0.9992 - val_loss: 0.9764\n",
      "Epoch 8/100\n",
      "58913/58913 [==============================] - 1s 14us/step - loss: 0.9992 - val_loss: 0.9764\n",
      "Epoch 9/100\n",
      "58913/58913 [==============================] - 1s 14us/step - loss: 0.9992 - val_loss: 0.9764\n",
      "Epoch 10/100\n",
      "58913/58913 [==============================] - 1s 13us/step - loss: 0.9992 - val_loss: 0.9763\n",
      "Epoch 11/100\n",
      "58913/58913 [==============================] - 1s 14us/step - loss: 0.9992 - val_loss: 0.9763\n",
      "Epoch 12/100\n",
      "58913/58913 [==============================] - 1s 14us/step - loss: 0.9992 - val_loss: 0.9763\n",
      "Epoch 13/100\n",
      "58913/58913 [==============================] - 1s 14us/step - loss: 0.9992 - val_loss: 0.9763\n",
      "Epoch 14/100\n",
      "58913/58913 [==============================] - 1s 14us/step - loss: 0.9992 - val_loss: 0.9763\n",
      "Epoch 15/100\n",
      "58913/58913 [==============================] - 1s 14us/step - loss: 0.9992 - val_loss: 0.9763\n",
      "Epoch 16/100\n",
      "58913/58913 [==============================] - 1s 15us/step - loss: 0.9992 - val_loss: 0.9763\n",
      "Epoch 17/100\n",
      "58913/58913 [==============================] - 1s 15us/step - loss: 0.9992 - val_loss: 0.9763\n",
      "Epoch 18/100\n",
      "58913/58913 [==============================] - 1s 14us/step - loss: 0.9992 - val_loss: 0.9763\n",
      "Epoch 19/100\n",
      "58913/58913 [==============================] - 1s 14us/step - loss: 0.9992 - val_loss: 0.9763\n",
      "Epoch 20/100\n",
      "58913/58913 [==============================] - 1s 14us/step - loss: 0.9992 - val_loss: 0.9763\n",
      "Epoch 21/100\n",
      "58913/58913 [==============================] - 1s 14us/step - loss: 0.9992 - val_loss: 0.9763\n",
      "Epoch 22/100\n",
      "58913/58913 [==============================] - 1s 14us/step - loss: 0.9992 - val_loss: 0.9763\n",
      "Epoch 23/100\n",
      "58913/58913 [==============================] - 1s 14us/step - loss: 0.9992 - val_loss: 0.9763\n",
      "Epoch 24/100\n",
      "58913/58913 [==============================] - 1s 14us/step - loss: 0.9992 - val_loss: 0.9763\n",
      "Epoch 25/100\n",
      "58913/58913 [==============================] - 1s 13us/step - loss: 0.9992 - val_loss: 0.9763\n",
      "Epoch 26/100\n",
      "58913/58913 [==============================] - 1s 14us/step - loss: 0.9992 - val_loss: 0.9763\n",
      "Epoch 27/100\n",
      "58913/58913 [==============================] - 1s 14us/step - loss: 0.9992 - val_loss: 0.9763\n",
      "Epoch 28/100\n",
      "58913/58913 [==============================] - 1s 14us/step - loss: 0.9992 - val_loss: 0.9763\n",
      "Epoch 29/100\n",
      "58913/58913 [==============================] - 1s 14us/step - loss: 0.9992 - val_loss: 0.9763\n",
      "Epoch 30/100\n",
      "58913/58913 [==============================] - 1s 15us/step - loss: 0.9992 - val_loss: 0.9763\n",
      "Epoch 31/100\n",
      "58913/58913 [==============================] - 1s 15us/step - loss: 0.9992 - val_loss: 0.9763\n",
      "Epoch 32/100\n",
      "58913/58913 [==============================] - 1s 14us/step - loss: 0.9992 - val_loss: 0.9763\n",
      "Epoch 33/100\n",
      "58913/58913 [==============================] - 1s 14us/step - loss: 0.9992 - val_loss: 0.9763\n",
      "Epoch 34/100\n",
      "58913/58913 [==============================] - 1s 15us/step - loss: 0.9992 - val_loss: 0.9763\n",
      "Epoch 35/100\n",
      "58913/58913 [==============================] - 1s 14us/step - loss: 0.9992 - val_loss: 0.9763\n",
      "Epoch 36/100\n",
      "58913/58913 [==============================] - 1s 15us/step - loss: 0.9992 - val_loss: 0.9763\n",
      "Epoch 37/100\n",
      "58913/58913 [==============================] - 1s 14us/step - loss: 0.9992 - val_loss: 0.9763\n",
      "Epoch 38/100\n",
      "58913/58913 [==============================] - 1s 14us/step - loss: 0.9992 - val_loss: 0.9763\n",
      "Epoch 39/100\n",
      "58913/58913 [==============================] - 1s 14us/step - loss: 0.9992 - val_loss: 0.9763\n"
     ]
    }
   ],
   "source": [
    "hist = vae.fit(x_train, x_train,\n",
    "               shuffle=True,\n",
    "               epochs=epochs,\n",
    "               batch_size=batch_size,\n",
    "               validation_data=(x_val, x_val),\n",
    "               callbacks=my_callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x266.991 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "golden_size = lambda width: (width, 2. * width / (1 + np.sqrt(5)))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=golden_size(6))\n",
    "\n",
    "hist_df = pd.DataFrame(hist.history)\n",
    "hist_df.plot(ax=ax)\n",
    "\n",
    "ax.set_ylabel('NELBO')\n",
    "ax.set_xlabel('# epochs')\n",
    "\n",
    "ax.set_ylim(.99*hist_df[1:].values.min(), \n",
    "            1.1*hist_df[1:].values.max())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = vae.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7364, 400)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (7364,100,4) (7364,400) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-d12297323133>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdistr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (7364,100,4) (7364,400) "
     ]
    }
   ],
   "source": [
    "distr = (x_test - predictions) / x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pT_pred = [row[0] for row in predictions]\n",
    "eta_pred = [row[1] for row in predictions]\n",
    "phi_pred = [row[2] for row in predictions]\n",
    "mass_pred = [row[3] for row in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pT = [row[0] for row in x_test]\n",
    "eta = [row[1] for row in x_test]\n",
    "phi = [row[2] for row in x_test]\n",
    "mass = [row[3] for row in x_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking output (orange) over input (blue) for pT, eta, phi, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(-3, 3, 80)\n",
    "plt.hist(pT, bins=bins,alpha=0.5)\n",
    "plt.hist(pT_pred, bins=bins,alpha=0.5)\n",
    "bins = np.linspace(-0.45, -0.2, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(-5, 5, 100)\n",
    "plt.hist(eta, bins=bins,alpha=0.5)\n",
    "plt.hist(eta_pred, bins=bins,alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(-4,4, 100)\n",
    "plt.hist(phi, bins=bins,alpha=0.5)\n",
    "plt.hist(phi_pred, bins=bins,alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(-1,1, 60)\n",
    "plt.hist(mass, bins=bins,alpha=0.5)\n",
    "plt.hist(mass_pred, bins=bins,alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_pT = [row[0] for row in distr]\n",
    "d_eta = [row[1] for row in distr]\n",
    "d_phi = [row[2] for row in distr]\n",
    "d_mass = [row[3] for row in distr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of (input - output) / input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(-2, 2, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(d_pT, bins=bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(d_eta, bins=bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(d_phi, bins=bins)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
